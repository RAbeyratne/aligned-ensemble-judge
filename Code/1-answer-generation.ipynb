{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a9382-1d0f-442b-b8c1-dff14d593012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import seaborn as sns\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e81ee-d229-47d7-bdcd-cc2bfbeeef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_L = \"XXX\"\n",
    "login(token=HF_L)\n",
    "\n",
    "model = 'llama'\n",
    "\n",
    "if model == 'llama':\n",
    "    llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=HF_L)\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llama_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        use_auth_token=HF_L\n",
    "    )\n",
    "if model == 'falcon':\n",
    "    falcon_model = \"tiiuae/falcon-7b-instruct\"\n",
    "    falcon_tokenizer = AutoTokenizer.from_pretrained(falcon_model)\n",
    "    falcon_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=falcon_model,\n",
    "        tokenizer=falcon_tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "if model == 'gemma':\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-7b-it\", \n",
    "        device_map=\"auto\", \n",
    "        revision=\"float16\")\n",
    "if model == 'mistral':\n",
    "    device = \"cuda\"\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac042a-57a9-4ca3-a515-7523d4b34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_llama(prompt, max_new_tokens=500, temperature=1, top_k=50, top_p=0.9, seed=42):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.manual_seed(seed)\n",
    "    outputs = llama_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    response = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def request_falcon(prompt, max_new_tokens=1000, temperature=1, top_k=50, top_p=0.9, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    sequences = falcon_pipeline(\n",
    "        prompt,\n",
    "        max_length=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,            \n",
    "        top_p=top_p,   \n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=falcon_tokenizer.eos_token_id,\n",
    "    )\n",
    "    result = sequences[0][\"generated_text\"]\n",
    "    generated_text = result[len(prompt):].strip() \n",
    "    return generated_text\n",
    "\n",
    "def request_gemma(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gemma_model.to(device)\n",
    "    input_ids = gemma_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    generated_text = gemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "def request_mistral(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mistral_model.to(device)\n",
    "    encodeds = mistral_tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = mistral_model.generate(\n",
    "            model_inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    decoded = mistral_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded[0].strip().replace('[INST] ', '').replace('[/INST] ', '')[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14cc01-6013-45d4-9c3c-70fef994a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_GENERATION_PROMPT = '''Generate an answer to the below question based on the provided snippet.\n",
    "\n",
    "question: \"{0}\"\n",
    "snippet: \"{1}\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dc581-173d-4d7f-853d-1345f8eb06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Ramitha/alqa-master-40\")\n",
    "df = pd.DataFrame(dataset['rawcases'])\n",
    "df.info()\n",
    "\n",
    "num_points = 21\n",
    "step = 3.0 / (num_points - 1)\n",
    "temperature_values = [round(i * step, 2) for i in range(num_points)]\n",
    "temperature_values = temperature_values[1:] \n",
    "print(temperature_values)\n",
    "\n",
    "cumulative_df = pd.DataFrame()\n",
    "for temp in tqdm(temperature_values):\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        snippet = row['snippet']\n",
    "        prompt = ANSWER_GENERATION_PROMPT.format(question, snippet)\n",
    "        temperature = temp\n",
    "        \n",
    "        if model == 'llama':\n",
    "            response = request_llama(prompt, temperature=temp)\n",
    "        elif model == 'falcon':\n",
    "            response = request_falcon(prompt, temperature=temp)\n",
    "        elif model == 'gemma':\n",
    "            response = request_gemma(prompt, temperature=temp)\n",
    "        elif model == 'mistral':\n",
    "            response = request_mistral(prompt, temperature=temp)\n",
    "        \n",
    "        new_row = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'snippet': snippet,\n",
    "            'answerGenerated': response,\n",
    "            'temperature': temperature,\n",
    "            'model': model\n",
    "        }\n",
    "        cumulative_df = pd.concat([cumulative_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    'rawcases': Dataset.from_pandas(cumulative_df)\n",
    "})\n",
    "hf_dataset.push_to_hub(\"Ramitha/alqa-results-40-\" + model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8813802-f398-4f0e-bb96-4724ac07d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Ramitha/sl-master-40\")\n",
    "df = pd.DataFrame(dataset['rawcases'])\n",
    "df.info()\n",
    "\n",
    "num_points = 21\n",
    "step = 3.0 / (num_points - 1)\n",
    "temperature_values = [round(i * step, 2) for i in range(num_points)]\n",
    "temperature_values = temperature_values[1:] \n",
    "print(temperature_values)\n",
    "\n",
    "cumulative_df = pd.DataFrame()\n",
    "for temp in tqdm(temperature_values):\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        snippet = row['snippet']\n",
    "        prompt = ANSWER_GENERATION_PROMPT.format(question, snippet)\n",
    "        temperature = temp\n",
    "        \n",
    "        if model == 'llama':\n",
    "            response = request_llama(prompt, temperature=temp)\n",
    "        elif model == 'falcon':\n",
    "            response = request_falcon(prompt, temperature=temp)\n",
    "        elif model == 'gemma':\n",
    "            response = request_gemma(prompt, temperature=temp)\n",
    "        elif model == 'mistral':\n",
    "            response = request_mistral(prompt, temperature=temp)\n",
    "        \n",
    "        new_row = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'snippet': snippet,\n",
    "            'answerGenerated': response,\n",
    "            'temperature': temperature,\n",
    "            'model': model\n",
    "        }\n",
    "        cumulative_df = pd.concat([cumulative_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    'rawcases': Dataset.from_pandas(cumulative_df)\n",
    "})\n",
    "hf_dataset.push_to_hub(\"Ramitha/sl-results-40-\" + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6aaab-2a9e-4af0-afe6-a986a215ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
